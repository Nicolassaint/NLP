{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8iwPmgjhJEd"
      },
      "source": [
        "# Training a character language model and studying various ways of generating text\n",
        "\n",
        "**Author: matthieu.labeau@telecom-paris.fr**\n",
        "\n",
        "## Objectives:\n",
        "\n",
        "- We will train a network to predict a next character given an input sequence of characters, and use it to generate new sequences.\n",
        "- We will strictly work with local (and not structured) prediction - however, we will look into relatively simple heuristics to improve the \"structure\" of our generation: *temperature* sampling, *beam search*, *top-k* sampling, *top-p* sampling, drop-out input data.\n",
        "- We will use ```keras```to build the model based on a LSTM, which will use simple features (one-hot vector representing previous characters) to predict the next characters. We will use a small model to avoid training for too long.\n",
        "- We will use a small dataset (poetry, from project Gutenberg) - you can use any data you prefer, as long as you are able to train the model on it.\n",
        "- Even with a small dataset and a small model, training may be long (~1 min/epoch on GPU). If you can use a computing infrastructure, like Google colab, it may be more practical - and you probably can obtain better results by using a bigger model and a larger dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bIITyO78agJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWvSyqKyhJEk"
      },
      "source": [
        "### Obtaining the data\n",
        "- We download directly the ebook from project Gutenberg - you can get any other text you would prefer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZFtJnsqhJEo"
      },
      "outputs": [],
      "source": [
        "from keras.utils import get_file\n",
        "url = 'https://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
        "path = get_file('pg100.txt', origin=url)\n",
        "\n",
        "f = open(path, 'r' , encoding = 'utf8')\n",
        "lines = f.readlines()\n",
        "text = []\n",
        "\n",
        "start = False\n",
        "for line in lines:\n",
        "    line = line.strip().lower() # Remove blanks and capitals\n",
        "    if(\"*** START OF THE PROJECT GUTENBERG EBOOK THE COMPLETE WORKS OF WILLIAM SHAKESPEARE ***\".lower() in line and start==False):\n",
        "        start = True\n",
        "    if(\"*** END OF THE PROJECT GUTENBERG EBOOK THE COMPLETE WORKS OF WILLIAM SHAKESPEARE ***\".lower() in line):\n",
        "        break\n",
        "    if(start==False or len(line) == 0):\n",
        "        continue\n",
        "    text.append(line)\n",
        "\n",
        "f.close()\n",
        "text = \" \".join(text)\n",
        "voc_chars = sorted(set([c for c in text]))\n",
        "nb_chars = len(voc_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq9rNoU5hJEs"
      },
      "outputs": [],
      "source": [
        "print(lines[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZbgX7SZhJEu"
      },
      "source": [
        "### Keeping track of possible characters\n",
        "- Using a ```set```, create a sorted list of possible characters\n",
        "- Create two dictionnaries, having characters and corresponding indexes as {key: value}, and reverse.\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "chars = [a, b, c]\n",
        "```\n",
        "\n",
        "```python\n",
        "chars_indices = {a: 0, b: 1, c: 2}\n",
        "```\n",
        "\n",
        "```python\n",
        "indices_chars = {0: a, 1: b, 2: c}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udNyLiiEhJEw"
      },
      "outputs": [],
      "source": [
        "print('Corpus length:', len(text))\n",
        "\n",
        "chars = ...\n",
        "print('Total number of characters:', len(chars))\n",
        "char_indices = ...\n",
        "indices_char = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWytAwT2iwjf"
      },
      "source": [
        "#### Parenthesis: looking at words\n",
        "\n",
        "- Try to look at tokenization schemes: what are the most frequent words if we use *whitespace* tokenization ?\n",
        "- What are the most frequent words in this dataset using smarter tokenization ? You can a tokenizer from NLTK: ```nltk.word tokenize```.\n",
        "- Make a plot of rank vs. word count. Does Zipf's Law seem to hold ?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yxbc7YZQ2rsu"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "#Â Apply whitespace tokenization\n",
        "tokens = ...\n",
        "\n",
        "# Get frequencies and sort words according to them\n",
        "freq = ...\n",
        "ordered_word_list = ...\n",
        "print(ordered_word_list[:100])\n",
        "\n",
        "del tokens\n",
        "del freq\n",
        "del ordered_word_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO1FmkY95Xyd"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53QEqxMF3KXd"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize\n",
        "tokens = ...\n",
        "\n",
        "freq = ...\n",
        "ordered_word_list = ...\n",
        "print(ordered_word_list[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK8TRLd6kyRv"
      },
      "outputs": [],
      "source": [
        "# Create an array containing the word rank in the first dimension, and its count in the second.\n",
        "rank_counts = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YxWvrN12fIz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "plt.title('Word counts versus rank')\n",
        "plt.scatter(rank_counts[:,0], rank_counts[:,1])\n",
        "plt.yscale('log')\n",
        "plt.show()\n",
        "\n",
        "print('Vocabulary size: %i' % len(freq))\n",
        "print('Part of the corpus by taking the \"x\" most frequent words:')\n",
        "for i in range(1000, len(freq), 1000):\n",
        "    print('%i : %.2f' % (i, np.sum(rank_counts[:i, 1]) / np.sum(rank_counts[:,1]) ))\n",
        "\n",
        "del tokens\n",
        "del freq\n",
        "del ordered_word_list\n",
        "del rank_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBvvgFLUhJEy"
      },
      "source": [
        "### Creating a model: a first, simple version\n",
        "\n",
        "This model will take as input **a fixed number of characters** and output the next one. We will treat the model as a *black box* and leave its innner working for later.\n",
        "\n",
        "#### Creating training data\n",
        "- We will represent characters using *one-hot vectors*. Hence, the i-th character of n possible characters will be represented by a vector of length $n$, containing $0$ expect for a $1$ in position $i$. Following our previous examples, ```a = [1, 0, 0]``` and ```b = [0, 1, 0]```.\n",
        "- Hence, a sequence of characters is a list of one-hot vectors. Our goal will be to predict, given an input sequence of fixed length (here, this length is given by ```maximum_seq_length```) the next character. Hence, we need to build two lists: ```sentences```, containing the input sequences, and ```next_char``` the characters to be predicted.\n",
        "- We do not necessarily need to take all possible sequences. We can select one every ```time_step``` steps.\n",
        "\n",
        "Example: Using the previous dictionnaries, the sequence:\n",
        "```'acabbaccaabba'``` with ```maximum_seq_length = 4``` and ```time_step = 2``` would give the following lists:\n",
        "\n",
        "```python\n",
        "sentences = ['acab', 'abba', 'bacc', 'ccaa', 'aabb']\n",
        "```\n",
        "\n",
        "```python\n",
        "next_char = ['b', 'c', 'a', 'b', 'a']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5mJDBLchJEz"
      },
      "outputs": [],
      "source": [
        "maximum_seq_length = 30\n",
        "time_step = 4\n",
        "sentences = []\n",
        "next_char = []\n",
        "for i in range(...):\n",
        "    sentences.append(...)\n",
        "    next_char.append(...)\n",
        "print('Number of Sequences:', len(sentences))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9dG3M9ahJE2"
      },
      "source": [
        "#### Creating training tensors\n",
        "- We need to transform these lists into tensors, using one-hot vectors to represent characters.\n",
        "- We will need 3 dimensions for the training examples from ```sentences```: the number of examples, the length of the sequence, and the dimension of the one-hot vector\n",
        "- This is reduced to 2 dimensions for the ```next_char```: number of examples and one-hot vector.\n",
        "\n",
        "Example: the previous ```sentences``` would become:\n",
        "\n",
        "```python\n",
        "X = [[[1, 0, 0],\n",
        "      [0, 0, 1],\n",
        "      [1, 0, 0],\n",
        "      [0, 1, 0]],\n",
        "     [[1, 0, 0],\n",
        "      [0, 1, 0],\n",
        "      [0, 1, 0],\n",
        "      [1, 0, 0]],\n",
        "     [[0, 1, 0],\n",
        "      [1, 0, 0],\n",
        "      [0, 0, 1],\n",
        "      [0, 0, 1]],\n",
        "     [[0, 0, 1],\n",
        "      [0, 0, 1],\n",
        "      [1, 0, 0],\n",
        "      [1, 0, 0]],\n",
        "     [[1, 0, 0],\n",
        "      [1, 0, 0],\n",
        "      [0, 1, 0],\n",
        "      [0, 1, 0]]]\n",
        "```\n",
        "       \n",
        "```python\n",
        "y = [[0, 1, 0],\n",
        "     [0, 0, 1],\n",
        "     [1, 0, 0],\n",
        "     [0, 1, 0],\n",
        "     [1, 0, 0]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCXjwGtahJE3"
      },
      "outputs": [],
      "source": [
        "X = np.zeros((len(sentences), maximum_seq_length, len(chars)), dtype=bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=bool)\n",
        "# Loop over the sentences\n",
        "for ...\n",
        "    #Â Loop over the characters\n",
        "    for ...\n",
        "        # Put the right value of X to 1\n",
        "        ...\n",
        "    # Put the right value of y to 1\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktQugjFcFUHs"
      },
      "source": [
        "#### Implement the model\n",
        "\n",
        "In order to implement the model as simply as possible, we will use ```keras```. It allows to create models with only a few lines of code.\n",
        "First, we will create a very simple model based on a **LSTM**, which is a *recurrent* architecture. Note that one the strength of a recurrent architecture is to allow for inputs of varying length - here, to simplify data processing, we will keep a **fixed input size**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzIGLB9_hJE5"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import LambdaCallback, EarlyStopping\n",
        "from keras.metrics import sparse_categorical_crossentropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqKe6y1ShJE5"
      },
      "source": [
        "We need to create a LSTM model that takes directly out inputs from ```X``` and try to predict one-hot vectors from ```y```.\n",
        "- What are the input and output dimensions ?\n",
        "  - ```X```: size of the dataset $\\times$ maximum sequence length $\\times$ vocabulary size\n",
        "  - ```y```: size of the dataset $\\times$ vocabulary size\n",
        "- The model should be made with a ```LSTM``` layer, and a ```Dense``` layer followed by a softmax activation function. Work out the intermediate dimensions:\n",
        "  - ```X``` $\\rightarrow$ (LSTM) $\\rightarrow$ ```h``` $\\rightarrow$ (Dense) $\\rightarrow$ ```s``` $\\rightarrow$ (softmax) $\\rightarrow$ ```pred```\n",
        "  - Look at layers arguments and find out to proper ```input_shape``` for the ```LSTM``` layer and the proper size for the ```Dense``` layer.\n",
        "  - You can use 128 as the size of hidden states for the ```LSTM```.\n",
        "- We will minimize ```cross-entropy(pred, y)```. Use the ```categorical_crossentropy``` loss, with the optimizer of your preference (for example, ```adam```)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmrllaH_hJE6"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maximum_seq_length, len(chars))))\n",
        "model.add(Dense(len(chars)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3nlMjqGHCYc"
      },
      "source": [
        "You will now only need a few functions to use this model:\n",
        "- ```model.fit```, which you will call on the appropriately processed data ```X, y```\n",
        "- ```model.predict```, which you will use on an input **of the same dimension of X** to output the probabilities. That includes the *first one*, corresponding to the number of examples in the input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5jZ9jv3hJE7"
      },
      "source": [
        "### Create functions to generate text with our model\n",
        "- We use the output of our model (vector of probabilities on characters) to select the next most probable character (with the ```argmax```)\n",
        "- We need to transform an input text into an input tensor, as before (taking the right length, the last ```maximum_seq_length``` characters)\n",
        "- We need to transform back the most probable index into a character and add it to our text.\n",
        "- This must be looped ```num_generated``` times, each time obtaining a new input tensor from the new input sequence (which has the character we previously predicted at the end !)\n",
        "\n",
        "We can begin to write a function facilitating the transfer between text and tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeiDQDnqJSwu"
      },
      "outputs": [],
      "source": [
        "def get_tensor(sentence, maximum_seq_length, voc):\n",
        "    x = np.zeros(...)\n",
        "    # Fill out x appropriately\n",
        "    ...\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOepuasUJS58"
      },
      "source": [
        "You have now what is necessary to fill out ```generate_next```.\n",
        "\n",
        "The following function (```end_epoch_generate```) is here to facilitate automatic generation at the end of each epoch, so you can monitor of generation changes as the model trains. It calls the ```generate_next``` function upon each sequence of text in ```texts_ex```. The only element in this list right now comes from the training data - you can add your own. We also use ```EarlyStopping``` to stop training when validation loss does not decrease."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMBXDllYhJE7"
      },
      "outputs": [],
      "source": [
        "def generate_next(model, text, num_generated=120):\n",
        "    generated = text\n",
        "    sentence = text[-maximum_seq_length:]\n",
        "    for i in range(num_generated):\n",
        "        # Obtain the representation for the sentence, the prediction, the index of the better character and the character itself.\n",
        "        x = ...\n",
        "        predictions = ...\n",
        "        next_index = ...\n",
        "        next_char = ...\n",
        "        generated += next_char\n",
        "        sentence = sentence[1:] + next_char\n",
        "    return(generated)\n",
        "\n",
        "def end_epoch_generate(epoch, _):\n",
        "    print(' Generating text after epoch: %d' % (epoch+1))\n",
        "    texts_ex = [\"From fairest creatures we desire increase,\"]\n",
        "    for text in texts_ex:\n",
        "        sample = generate_next(model, text.lower())\n",
        "        print('%s' % (sample))\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0,\n",
        "    patience=2,\n",
        "    verbose=0,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIp50FeYhJE8"
      },
      "source": [
        "Test generation with the model not yet trained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "583GgxOGhJE8"
      },
      "outputs": [],
      "source": [
        "text_ex = \"From fairest creatures we desire increase,\"\n",
        "generate_next(model, text_ex.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcQJvVZyhJE9"
      },
      "outputs": [],
      "source": [
        "model.fit(X, y,\n",
        "          batch_size=128,\n",
        "          epochs=5,\n",
        "          validation_split = 0.2,\n",
        "          callbacks=[LambdaCallback(on_epoch_end=end_epoch_generate), early_stopping])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUeWKl-_hJE9"
      },
      "source": [
        "#### Sampling with our model\n",
        "- Now, instead of simply selecting the most probable next character, we would like to be able to draw a sample from the distribution output by the model.\n",
        "- To better control the generation, we would like to use the argument ```temperature```, to smooth the distribution.\n",
        "- Use the ```multinomial``` function from the ```random``` package to draw samples.\n",
        "- Integrate this into a function ```generate_sample``` that be almost exactly like ```generate_next```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCQBbrIghJE-"
      },
      "outputs": [],
      "source": [
        "def sample(predictions, temperature, placeholder=None):\n",
        "    # From the prediction, apply the temperature to reweights the probabilities and sample.\n",
        "    ...\n",
        "    probas = ...\n",
        "    return ...\n",
        "\n",
        "def generate_sample(model, text, sample_function, *sampling_arg, num_generated=120, temperature=1.0):\n",
        "    generated = text\n",
        "    sentence = text[-maximum_seq_length:]\n",
        "    for i in range(num_generated):\n",
        "        x = ...\n",
        "        predictions = ...\n",
        "        next_index = sample_function(predictions, temperature, *sampling_arg)\n",
        "        next_char = ...\n",
        "        generated += next_char\n",
        "        sentence = sentence[1:] + next_char\n",
        "    return(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtwRJag3hJE-"
      },
      "outputs": [],
      "source": [
        "generate_sample(model, text_ex.lower(), sample, temperature = 0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOeuhvq_hJE_"
      },
      "source": [
        "#### Generate text with the beam algorithm\n",
        "- Complete this function implementing the beam procedure.\n",
        "- We need to loop for each character we want to generate, keeping track of the best ```beam_size``` sequences at the most.\n",
        "- Besides keeping track of past generated character for each of these ```beam_size``` sequences, we need to keep track of their log-probability.\n",
        "- This is done by, at each loop, keeping the ```beam_size```best predictions for each of the ```beam_size``` sequences, computing the log-probabilities of the newly formed (```beam_size```)$^2$ , and keeping the overall ```beam_size``` best new sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRM0L4S6hJE_"
      },
      "outputs": [],
      "source": [
        "def generate_beam(model, text, beam_size=5, num_generated=120):\n",
        "    generated = text\n",
        "    sentence = text[-maximum_seq_length:]\n",
        "    # Initialization of the beam with log-probabilities for the sequence\n",
        "    current_beam = [(0, [], sentence)]\n",
        "\n",
        "    for l in range(num_generated):\n",
        "        all_beams = []\n",
        "        for prob, current_preds, current_input in current_beam:\n",
        "            x = ...\n",
        "            prediction = ...\n",
        "            # beam_size best predictions !\n",
        "            possible_next_chars = ...\n",
        "            # Add to the beams: (the probability of the sequence, the sequence of indexes generated, the full sequence of characters)\n",
        "            all_beams += [\n",
        "                (...,\n",
        "                 ...,\n",
        "                 ...\n",
        "                )\n",
        "                for ... in possible_next_chars]\n",
        "\n",
        "        # Sort the beams according to their probability and keep the beam_size best\n",
        "        current_beam = ...\n",
        "\n",
        "    return ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVNP5NHFhJFA"
      },
      "outputs": [],
      "source": [
        "generate_beam(model, text_ex.lower())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMTmgVHbhJFA"
      },
      "source": [
        "#### Generate text with top-k sampling\n",
        "- This is very similar to the previously implemented sampling procedure, but we would like to choose a parameter ```k```, which is used to limit the sampling to the top-$k$ results of the model prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuxJroHghJFA"
      },
      "outputs": [],
      "source": [
        "def sample_top_k(predictions, temperature, k):\n",
        "    ...\n",
        "    indices_to_remove = ...\n",
        "    predictions[indices_to_remove] = -float('Inf')\n",
        "    ...\n",
        "\n",
        "    return ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oP3vPctshJFA"
      },
      "outputs": [],
      "source": [
        "generate_sample(model, text_ex.lower(), sample_top_k, 5, temperature = 0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gynfp2huhJFB"
      },
      "source": [
        "#### Generate text with top-p sampling\n",
        "- This is very similar to the previously implemented sampling procedure, but we would like to choose a parameter ```p```, which is used to limit the sampling to the top results of the model prediction corresponding together to at least the probability $p < 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xefxxBLwhJFB"
      },
      "outputs": [],
      "source": [
        "def sample_top_p(predictions, temperature, p):\n",
        "    ...\n",
        "\n",
        "    cum_prob = 0.0\n",
        "    incr = 0\n",
        "    # Get indices in increasing order of probability\n",
        "    indices = ...\n",
        "    probs = predictions[indices]\n",
        "    # Increment cum_prob with until it gets above 1 - p\n",
        "    # Keep track of the indices, which will be those to remove\n",
        "    while ...\n",
        "      ...\n",
        "\n",
        "    indices_to_remove = indices[incr:]\n",
        "    log_predictions[indices_to_remove] = -float('Inf')\n",
        "    ...\n",
        "    return ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXwnNLK5hJFC"
      },
      "outputs": [],
      "source": [
        "generate_sample(model, text_ex.lower(), sample_top_p, 0.75, temperature = 0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_wrNn_erDZ9"
      },
      "source": [
        "#### Reranking\n",
        "If we would like to use our outputs for a task, a common strategy is to generate a large number of them, and select the one that maximizes the relevant metric: this is **re-ranking**.\n",
        "\n",
        "First, we need an evaluation measure. For now, we can use **perplexity**: given how cross-entropy is computed in ```keras```, perplexity is simply *the exponential of the mean cross-entropy over the sequence*.\n",
        "\n",
        "To compute the cross-entropy, you need both the true ```y``` and the corresponding ```pred``` output by the model. While the most efficient would be to gather these while sampling, we can re-compute it for any sentence with the following function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wT2OGm9dwFCX"
      },
      "outputs": [],
      "source": [
        "def get_preds(sentence):\n",
        "    # Reconstitute the inputs and outputs from the sentence: first as list of characters\n",
        "    eval_input = []\n",
        "    eval_target = []\n",
        "    for i in range(0, len(sentence) - maximum_seq_length):\n",
        "        eval_input.append(sentence[i: i + maximum_seq_length])\n",
        "        eval_target.append(sentence[i + maximum_seq_length])\n",
        "\n",
        "    # Then as tensors\n",
        "    eval_X = np.zeros(...)\n",
        "    for ...\n",
        "        for ...\n",
        "            ...\n",
        "    ...\n",
        "\n",
        "    eval_pred = ...\n",
        "    return eval_y, eval_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f_CnqvG2ypO"
      },
      "outputs": [],
      "source": [
        "def perplexity(y_true, y_pred):\n",
        "    cross_entropy = ...\n",
        "    perplexity = ...\n",
        "    return perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMRrMzcuwFpW"
      },
      "outputs": [],
      "source": [
        "def re_rank(sentence):\n",
        "    eval_y, eval_pred = get_preds(sentence)\n",
        "    return perplexity(eval_y, eval_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGv4qxltbOVK"
      },
      "source": [
        "You can then generate a bunch of sentences, obtain their perplexity and sort them accordingly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VWHb5nG9_t8"
      },
      "outputs": [],
      "source": [
        "reranked = dict()\n",
        "for k in np.arange(5,10):\n",
        "    for t in np.arange(0.5, 1.0, 0.05):\n",
        "        sentence = generate_sample(model, text_ex.lower(), sample_top_k, k, temperature = 0.7)\n",
        "        p = re_rank(sentence)\n",
        "        reranked[sentence] = p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQd_5M6o4tcp"
      },
      "outputs": [],
      "source": [
        "print(sorted(reranked, key=reranked.get))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFR9563Z5qz_"
      },
      "outputs": [],
      "source": [
        "print(reranked)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LADfuc6w8ohV"
      },
      "outputs": [],
      "source": [
        "del X\n",
        "del y\n",
        "del model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEPEjdxbm2N6"
      },
      "source": [
        "### Create a second model: with embeddings\n",
        "\n",
        "This model will have a key difference with the previous one: we will use **dense embeddings** instead of one-hot representations.\n",
        "This means that:\n",
        "- In ```X```, we will use the index of the character instead of an one-hot vector. For example, the following ```['acab', 'abba', 'bacc', 'ccaa', 'aabb']``` will be represented as ```[[0, 2, 0, 1], [0, 1, 1, 0], [1, 0, 2, 2], [2, 2, 0, 0]]```.\n",
        "- We will need another input layer before the LSTM. Instead of ```Dense```, keras has a dedicated layer: ```Embedding```.\n",
        "\n",
        "Create the dataset, adapt the generation function, create the new model, and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WmQ-uetVJk9"
      },
      "outputs": [],
      "source": [
        "X_emb = np.zeros(...)\n",
        "y_emb = np.zeros(...)\n",
        "for ...\n",
        "    for ...\n",
        "        ...\n",
        "    ...\n",
        "\n",
        "print('X_emb shape:', X_emb.shape)\n",
        "print('y_emb shape:', y_emb.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EY0SoWgzcsd"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7qNLscWhJFE"
      },
      "outputs": [],
      "source": [
        "def generate_next_emb(model, text, num_generated=120):\n",
        "    generated = text\n",
        "    sentence = text[-maximum_seq_length:]\n",
        "    char_idxs = [[char_indices[char] for char in sentence]]\n",
        "    for i in range(num_generated):\n",
        "        x = ...\n",
        "        predictions = ...\n",
        "        next_index = ...\n",
        "        next_char = ...\n",
        "        generated += next_char\n",
        "        char_idxs = [char_idxs[0][1:] + [next_index]]\n",
        "    return(generated)\n",
        "\n",
        "def end_epoch_generate(epoch, _):\n",
        "    print('\\n Generating text after epoch: %d' % (epoch+1))\n",
        "    texts_ex = [\"From fairest creatures we desire increase,\"]\n",
        "    for text in texts_ex:\n",
        "        sample = generate_next_emb(model_emb_m2m, text)\n",
        "        print('%s' % (sample))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEGCsBpvhJFE"
      },
      "outputs": [],
      "source": [
        "model_emb_m2m = Sequential()\n",
        "model_emb_m2m.add(...)\n",
        "model_emb_m2m.add(LSTM(128, input_shape=(maximum_seq_length, 32), return_sequences=False))\n",
        "model_emb_m2m.add(Dense(len(chars)))\n",
        "model_emb_m2m.add(Activation('softmax'))\n",
        "\n",
        "model_emb_m2m.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzM17QC3hJFF"
      },
      "outputs": [],
      "source": [
        "model_emb_m2m.fit(X_emb, y_emb,\n",
        "                  batch_size=128,\n",
        "                  epochs=5,\n",
        "                  validation_split = 0.2,\n",
        "                  callbacks=[LambdaCallback(on_epoch_end=end_epoch_generate)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PidmpsnbhJFF"
      },
      "outputs": [],
      "source": [
        "generate_next_emb(model_emb_m2m, \"From fairest creatures we desire increase,\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KyLbTcem5qE"
      },
      "outputs": [],
      "source": [
        "def get_tensor_emb(sentence, maximum_seq_length, voc):\n",
        "    x = ...\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RthihH0u-CpJ"
      },
      "outputs": [],
      "source": [
        "def generate_sample_emb(model, text, sample_function, *sampling_arg, num_generated=120, temperature=1.0):\n",
        "    generated = text\n",
        "    sentence = text[-maximum_seq_length:]\n",
        "    for i in range(num_generated):\n",
        "        x = ...\n",
        "        predictions = ...\n",
        "        next_index = sample_function(predictions, temperature, *sampling_arg)\n",
        "        next_char =  ...\n",
        "        generated += next_char\n",
        "        sentence = sentence[1:] + next_char\n",
        "    return(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKJHS47L_FFQ"
      },
      "outputs": [],
      "source": [
        "generate_sample_emb(model_emb_m2m, text_ex.lower(), sample_top_k, 5, temperature = 0.7)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
