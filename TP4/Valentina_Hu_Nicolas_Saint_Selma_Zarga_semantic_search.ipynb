{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b17c85",
   "metadata": {},
   "source": [
    "# Lab: Semantic search on Question Answering datasets\n",
    "\n",
    "Groups:\n",
    "\n",
    "**Valentina HU**\n",
    "\n",
    "**Nicolas SAINT**\n",
    "\n",
    "**Selma ZARGA**\n",
    "\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "1. Explore and understand the **S**tanford **Qu**estion **A**nswering **D**ataset [Squad](https://aclanthology.org/D16-1264/) dataset and the associated task.  \n",
    "2. Adapt this dataset for a *local* semantic search task and propose an appropriate evaluation metric:\n",
    "    - Implement a simple baseline based on **TF-IDF**.\n",
    "    - Use a pre-trained transformer-based model, and fine-tune it.\n",
    "3. Test these approaches on the [CommonSense QA](https://aclanthology.org/N19-1421/) dataset. \n",
    "4. Adapt these approaches for a *global* semantic search task on the [WikiQA](https://aclanthology.org/D15-1237/) dataset for open domain question answering.\n",
    "5. **Bonus** (Optional) Apply a model (any, as long as it's running) to the original Squad QA task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c409670c",
   "metadata": {},
   "source": [
    "## Modalities:\n",
    "\n",
    "The goal of this lab is to make you search for and learn to use recent tools for NLP tasks. \n",
    "- You should feel free to use any tool, implementation and model you prefer. \n",
    "- You are not expected to reach a particular performance. \n",
    "- You can work on this lab by groups of up to 3. \n",
    "- You should submit this lab on the Moodle by Friday 22th."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80de1dc6",
   "metadata": {},
   "source": [
    "## 1 - The SQuAD dataset\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Questions:</div>\n",
    "\n",
    "1. Load the dataset **SQuAD** - for example, using the [```dataset``` package](https://huggingface.co/docs/datasets/index) from Huggingface and loading the dataset ```'squad'```. You can also explore it using the [website](https://rajpurkar.github.io/SQuAD-explorer/). \n",
    "2. Look at the metrics used to evaluate models on the dataset. You can also load the metric ```'squad'``` from the [```evaluate``` package](https://huggingface.co/docs/evaluate/index) from Huggingface. \n",
    "3. Explain succintly - and in your own words - what is the task: how could we use a model to solve it ? Treat the case of encoder models adapted to *classification tasks* and encoder-decoder models adapted to *text generation*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a93d09",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning' style='color:green;'>\n",
    "    Answer:\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e46e5425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 87599\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 10570\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a3a9f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric(name: \"squad\", features: {'predictions': {'id': Value(dtype='string', id=None), 'prediction_text': Value(dtype='string', id=None)}, 'references': {'id': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}}, usage: \"\"\"\n",
      "Computes SQuAD scores (F1 and EM).\n",
      "Args:\n",
      "    predictions: List of question-answers dictionaries with the following key-values:\n",
      "        - 'id': id of the question-answer pair as given in the references (see below)\n",
      "        - 'prediction_text': the text of the answer\n",
      "    references: List of question-answers dictionaries with the following key-values:\n",
      "        - 'id': id of the question-answer pair (see above),\n",
      "        - 'answers': a Dict in the SQuAD dataset format\n",
      "            {\n",
      "                'text': list of possible texts for the answer, as a list of strings\n",
      "                'answer_start': list of start positions for the answer, as a list of ints\n",
      "            }\n",
      "            Note that answer_start values are not taken into account to compute the metric.\n",
      "Returns:\n",
      "    'exact_match': Exact match (the normalized answer exactly match the gold answer)\n",
      "    'f1': The F-score of predicted tokens versus the gold answer\n",
      "Examples:\n",
      "\n",
      "    >>> predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]\n",
      "    >>> references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]\n",
      "    >>> squad_metric = datasets.load_metric(\"squad\")\n",
      "    >>> results = squad_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'exact_match': 100.0, 'f1': 100.0}\n",
      "\"\"\", stored examples: 0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metrics = load_metric(\"squad\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea2b9c6",
   "metadata": {},
   "source": [
    "**Remark :**\n",
    "\n",
    "The primary metric used to evaluate models on the SQuAD dataset is the **Exact Match (EM) score**. EM measures the percentage of predicted answers that match exactly with the ground truth answers. Another commonly used metric is the **F1 score**, which computes the overlap between predicted and true answers in terms of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2501b7aa",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a1ddd7",
   "metadata": {},
   "source": [
    "The SQuAD task is a question-answering (QA) task where the model is given a context paragraph and a question and needs to identify the answer span within the context paragraph. The dataset consists of passages from a variety of sources, and each passage is associated with multiple questions.\n",
    "\n",
    "- **Encoder Models for Classification Tasks**: In this setting, the model takes a context paragraph and a question as input and outputs the start and end indices of the answer span within the context. This can be formulated as a classification task, where the model is trained to predict whether each token in the context is the start or end of the answer span.\n",
    "\n",
    "- **Encoder-Decoder Models for Text Generation**: Alternatively, an encoder-decoder model could be used, where the context and question are encoded, and the model generates the answer span as a sequence of words. This approach involves training the model in a sequence-to-sequence fashion.\n",
    "\n",
    "The task involves understanding the context, comprehending the question, and precisely identifying the answer within the context. The success of the model is measured by its ability to produce accurate and exact answers. Thus, we could use the same metrics as mentionend before (EM & F1 scores).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba300531",
   "metadata": {},
   "source": [
    "## 2 - Design a *local* semantic search from squad\n",
    "\n",
    "This taks is a little complicated to implement. Let us simplify squad to be a **semantic search** task !\n",
    "We will divide the context containing the answer into several pieces, and ask a model to find which one contains the answer **by vectorizing the question and each piece** and trying to look for the most relevant piece using **cosine similarity** between the vectors, making it a fairly simple task.\n",
    "\n",
    "\n",
    "For example, the following question of the dataset:\n",
    "\n",
    "```python\n",
    "'Which NFL team represented the AFC at Super Bowl 50?'\n",
    "```\n",
    "\n",
    "with the answer:\n",
    "\n",
    "```python\n",
    "'Denver Broncos'\n",
    "```\n",
    "\n",
    "We could divide the corresponding ```'context'``` into the following list:\n",
    "\n",
    "```python\n",
    "['Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos d',\n",
    " \"efeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Franc\",\n",
    " 'isco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending',\n",
    " ' the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.']\n",
    "```\n",
    "\n",
    "and indicate the location of the answer as:\n",
    "\n",
    "```python\n",
    "label = [1, 0, 0, 0]\n",
    "```\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "            \n",
    "At first, we won't do any training: you should work with the ```validation``` part of the dataset. Be careful, there may be several good answers ! Propose a scheme to divide the context into pieces and to label each piece as containing the answer or not. How do we evaluate for this task - would simple accuracy suffice ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d68aea0",
   "metadata": {},
   "source": [
    "1) **Scheme to Divide Context into Pieces**\n",
    "\n",
    "To design a local semantic search from the SQuAD dataset, we can follow these steps:\n",
    "\n",
    "- **Divide Context into Pieces**: it breaks down the context into smaller, meaningful pieces. Each piece should be a coherent segment that contains relevant information.\n",
    "\n",
    "- **Vectorize Question and Context Pieces**: we can use a pre-trained transformer-based model (e.g., BERT, RoBERTa) to vectorize the question and each context piece. This involves obtaining embeddings for the question and each context piece.\n",
    "\n",
    "- **Cosine Similarity**: We can calculate the cosine similarity between the vectorized question and each vectorized context piece. Cosine similarity measures the cosine of the angle between two vectors and is commonly used to determine similarity.\n",
    "\n",
    "- **Select Most Relevant Piece**: Identifying the context piece with the highest cosine similarity as the predicted answer-containing piece.\n",
    "\n",
    "- **Labeling**: Creating labels for each context piece. The piece with the highest cosine similarity will be labeled as containing the answer, while others are labeled as not containing the answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e160c68f",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>\n",
    "            \n",
    "For efficient processing, you can use the ```map``` method associated to the dataset. It can create a new feature for each example. In this case, you can create a new feature containing the context divided into pieces, and a new feature containing labels for if the pieces contain the answer. You can also use it for your evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a35e093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Valen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a820c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_semantic(example):\n",
    "    # Split the context into sentences\n",
    "    sentences = nltk.sent_tokenize(example['context'])\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    all_text = sentences + [example['question']]\n",
    "    all_bow = vectorizer.fit_transform(all_text)\n",
    "\n",
    "    bow_sentences = all_bow[:-1].toarray()\n",
    "    bow_question = all_bow[-1].toarray()\n",
    "\n",
    "    sims = []\n",
    "    for i in range(bow_sentences.shape[0]):\n",
    "        bow_sent = bow_sentences[i]\n",
    "        sim = bow_sent @ bow_question.T / (np.linalg.norm(bow_sent.flatten()) * np.linalg.norm(bow_question.flatten()))\n",
    "        sims.append(sim)\n",
    "    \n",
    "    # Determine the index of the sentence with the highest similarity\n",
    "    max_index = np.argmax(sims) if sims else -1\n",
    "\n",
    "    # Create a label vector and assign the label to the sentence with the highest similarity\n",
    "    labels = [1 if i == max_index else 0 for i in range(len(sentences))]\n",
    "\n",
    "    return {'sentences': sentences, 'labels': labels}\n",
    "\n",
    "\n",
    "# Create a new dataset with the processed data\n",
    "processed_data = []\n",
    "processed_dataset = dataset['validation'].map(local_semantic, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "171a6a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '56be4db0acb8001400a502ec', 'title': 'Super_Bowl_50', 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'question': 'Which NFL team represented the AFC at Super Bowl 50?', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}, 'sentences': ['Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season.', 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title.', \"The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\", 'As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.'], 'labels': [0, 0, 0, 1]}\n",
      "{'id': '56be4db0acb8001400a502ed', 'title': 'Super_Bowl_50', 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'question': 'Which NFL team represented the NFC at Super Bowl 50?', 'answers': {'text': ['Carolina Panthers', 'Carolina Panthers', 'Carolina Panthers'], 'answer_start': [249, 249, 249]}, 'sentences': ['Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season.', 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title.', \"The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\", 'As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.'], 'labels': [0, 0, 0, 1]}\n",
      "{'id': '56be4db0acb8001400a502ee', 'title': 'Super_Bowl_50', 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'question': 'Where did Super Bowl 50 take place?', 'answers': {'text': ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"], 'answer_start': [403, 355, 355]}, 'sentences': ['Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season.', 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title.', \"The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\", 'As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.'], 'labels': [0, 0, 0, 1]}\n",
      "{'id': '56be4db0acb8001400a502ef', 'title': 'Super_Bowl_50', 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'question': 'Which NFL team won Super Bowl 50?', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}, 'sentences': ['Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season.', 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title.', \"The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\", 'As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.'], 'labels': [1, 0, 0, 0]}\n",
      "{'id': '56be4db0acb8001400a502f0', 'title': 'Super_Bowl_50', 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'question': 'What color was used to emphasize the 50th anniversary of the Super Bowl?', 'answers': {'text': ['gold', 'gold', 'gold'], 'answer_start': [488, 488, 521]}, 'sentences': ['Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season.', 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title.', \"The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\", 'As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.'], 'labels': [0, 0, 0, 1]}\n",
      "{'id': '56be8e613aeaaa14008c90d1', 'title': 'Super_Bowl_50', 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'question': 'What was the theme of Super Bowl 50?', 'answers': {'text': ['\"golden anniversary\"', 'gold-themed', '\"golden anniversary'], 'answer_start': [487, 521, 487]}, 'sentences': ['Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season.', 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title.', \"The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\", 'As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.'], 'labels': [1, 0, 0, 0]}\n",
      "{'id': '56be8e613aeaaa14008c90d2', 'title': 'Super_Bowl_50', 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'question': 'What day was the game played on?', 'answers': {'text': ['February 7, 2016', 'February 7', 'February 7, 2016'], 'answer_start': [334, 334, 334]}, 'sentences': ['Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season.', 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title.', \"The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\", 'As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.'], 'labels': [0, 0, 1, 0]}\n",
      "{'id': '56be8e613aeaaa14008c90d3', 'title': 'Super_Bowl_50', 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'question': 'What is the AFC short for?', 'answers': {'text': ['American Football Conference', 'American Football Conference', 'American Football Conference'], 'answer_start': [133, 133, 133]}, 'sentences': ['Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season.', 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title.', \"The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\", 'As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.'], 'labels': [1, 0, 0, 0]}\n",
      "{'id': '56bea9923aeaaa14008c91b9', 'title': 'Super_Bowl_50', 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'question': 'What was the theme of Super Bowl 50?', 'answers': {'text': ['\"golden anniversary\"', 'gold-themed', 'gold'], 'answer_start': [487, 521, 521]}, 'sentences': ['Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season.', 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title.', \"The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\", 'As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.'], 'labels': [1, 0, 0, 0]}\n",
      "{'id': '56bea9923aeaaa14008c91ba', 'title': 'Super_Bowl_50', 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'question': 'What does AFC stand for?', 'answers': {'text': ['American Football Conference', 'American Football Conference', 'American Football Conference'], 'answer_start': [133, 133, 133]}, 'sentences': ['Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season.', 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title.', \"The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\", 'As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.'], 'labels': [1, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "#print the first 10 examples\n",
    "for i in range(10):\n",
    "    print(processed_dataset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e993a4",
   "metadata": {},
   "source": [
    "Evaluating this model is complicated because the standard metrics used for SQuAD (like Exact Match and F1 score) rely on comparing predicted answer spans (start and end positions) with the actual spans. But our local semantic search return the most semantically similar sentence to the question, not a specific answer span.\n",
    "\n",
    "To evaluate the model we can create a simple accuracy model by counting every time the true answer is in our chosen sentences. This model has limits beause this method assumes the answer is a subset of a single sentence, which may not always be true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87a00d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56\n"
     ]
    }
   ],
   "source": [
    "def simple_accuracy_evaluation(dataset, num_samples=100):\n",
    "    correct = 0\n",
    "    for i in range(num_samples):\n",
    "\n",
    "        example = dataset[i]\n",
    "        answer = example['answers']['text'][0]  # get the first real answer\n",
    "        \n",
    "        # Get the predicted sentence by our local semantic search\n",
    "        predicted_sentences = example['sentences']\n",
    "        labels = example['labels']\n",
    "        predicted_sentence = predicted_sentences[labels.index(1)] if 1 in labels else \"\"\n",
    "\n",
    "        # Check if the answer is contained in the predicted sentence\n",
    "        if answer in predicted_sentence:\n",
    "            correct += 1\n",
    "\n",
    "    # Compute the accuracy\n",
    "    accuracy = correct / num_samples\n",
    "    return accuracy\n",
    "\n",
    "accuracy = simple_accuracy_evaluation(processed_dataset)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17dce092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.56\n"
     ]
    }
   ],
   "source": [
    "def simple_f1_evaluation(dataset, num_samples=100):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        example = dataset[i]\n",
    "        answer = example['answers']['text'][0]  # Get the first real answer\n",
    "\n",
    "        # Get the predicted sentence by our local semantic search\n",
    "        predicted_sentences = example['sentences']\n",
    "        labels = example['labels']\n",
    "        predicted_sentence = predicted_sentences[labels.index(1)] if 1 in labels else \"\"\n",
    "\n",
    "        # Check if the answer is contained in the predicted sentence\n",
    "        if answer in predicted_sentence:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_negatives += 1\n",
    "\n",
    "        # Check for false positives\n",
    "        if 1 in labels and not answer in predicted_sentence:\n",
    "            false_positives += 1\n",
    "\n",
    "    # Compute precision, recall, and F1 score\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    return f1_score\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_score = simple_f1_evaluation(processed_dataset)\n",
    "print(\"F1 Score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a24b9dd",
   "metadata": {},
   "source": [
    "The accuracy of our simple local semantic search is 0.56. Our prediction using BOW representaion as embedding is quite random. Bag Of Words model is a simplistic representation and disregard grammar and word order. There is therefore every reason to envisage a more complex embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df0d5ef",
   "metadata": {},
   "source": [
    "### 2.1 - Local search: Independant Tf-idf representations\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>\n",
    "            \n",
    "Implement a function that will for each example:\n",
    "- Create a tf-idf ```vectorizer``` from all the text in the question and context. \n",
    "- Create tf-idf representations for the question and the pieces of the context,\n",
    "- Find the representation the closest to the question among the pieces.\n",
    "\n",
    "Then, evaluate the method !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59dd3ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def tfidf_local_search(example):\n",
    "    # Get the previously computed sentences\n",
    "    sentences = nltk.sent_tokenize(example['context'])\n",
    "    question = example['question']\n",
    "\n",
    "    # Combine the question and sentences for vectorization\n",
    "    texts = [question] + sentences\n",
    "\n",
    "    # Create a tf-idf vectorizer and fit it on the texts\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "    # tf-idf representation for the question and sentences\n",
    "    question_vec = tfidf_matrix[0]\n",
    "    sentences_vecs = tfidf_matrix[1:]\n",
    "\n",
    "    # Compute cosine similarities between question and each sentence\n",
    "    similarities = cosine_similarity(question_vec, sentences_vecs)\n",
    "\n",
    "    # Find the index of the sentence with the highest similarity\n",
    "    most_similar_idx = similarities.argsort()[0][-1]\n",
    "\n",
    "    # Return the most similar sentence\n",
    "    return sentences[most_similar_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7a523c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Model Accuracy: 0.63\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "def evaluate_tfidf_accuracy(dataset, num_samples=100):\n",
    "    correct = 0\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        example = dataset[i]\n",
    "        answer = example['answers']['text'][0]  \n",
    "\n",
    "        # Use the tf-idf model to predict the sentence\n",
    "        predicted_sentence = tfidf_local_search(example)\n",
    "\n",
    "        # Check if the predicted sentence contains the answer\n",
    "        if answer in predicted_sentence:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / num_samples\n",
    "    return accuracy\n",
    "\n",
    "dataset = load_dataset(\"squad\", split='validation') \n",
    "accuracy = evaluate_tfidf_accuracy(dataset)\n",
    "print(\"TF-IDF Model Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46e4fb6",
   "metadata": {},
   "source": [
    "Keeping the same evaluation method as before, we try the TF-IDF representation. This embedding is more complex than BOW and consider word frequencies.\n",
    "As excepted, the accuracy is better. Hence, TF-IDF embedding captures better the relation between the words of our questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44498817",
   "metadata": {},
   "source": [
    "### 2.2 - Local search: Pre-trained sentence representations transformer-based model\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>\n",
    "\n",
    "Reproduce the same process using a pre-trained transformer model. You can use a model that you will find on huggingface. You can also look into the [```SentenceTransformer``` library](https://www.sbert.net/), dedicated to represent documents. Also:\n",
    "- Try to verify if the model has been trained on SQuAD !\n",
    "- Fine-tune the model (at least a little) to check that it improves results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b57ea350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, AdamW\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040c4710",
   "metadata": {},
   "source": [
    "We choose to use DistilBert because on the Hugging face website we saw some application of this model on the Squad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17e0daa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '56be4db0acb8001400a502ec', 'title': 'Super_Bowl_50', 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'question': 'Which NFL team represented the AFC at Super Bowl 50?', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}}\n",
      "Question: Which NFL team represented the AFC at Super Bowl 50?\n",
      "Answer: Denver Broncos\n",
      "Closest sentence: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def vectorize(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    return torch.mean(outputs.last_hidden_state, dim=1)[0]\n",
    "\n",
    "def transformer_local_search(question, sentences):\n",
    "    # Vectorize the question\n",
    "    question_vec = vectorize(question, tokenizer, model).detach().numpy()\n",
    "\n",
    "    # Vectorize sentences and compute cosine similarities\n",
    "    sims = []\n",
    "    for sent in sentences:\n",
    "        sent_vec = vectorize(sent, tokenizer, model).detach().numpy()\n",
    "        sim = 1 - cosine(sent_vec, question_vec)\n",
    "        sims.append(sim)\n",
    "\n",
    "    # Find the most similar sentence\n",
    "    return sentences[np.argmax(sims)]\n",
    "\n",
    "# Example usage\n",
    "example = dataset[0]\n",
    "print(example)\n",
    "\n",
    "print(\"Question:\", example['question'])\n",
    "print(\"Answer:\", example['answers']['text'][0])\n",
    "closest_sentence = transformer_local_search(example['question'], nltk.sent_tokenize(example['context']))\n",
    "print(\"Closest sentence:\", closest_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d67f0",
   "metadata": {},
   "source": [
    "#### Verify if the model has been trained on SQuAD !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "783a9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_question(size):\n",
    "    context = dataset['context'][:size]\n",
    "    question = dataset['question'][:size]\n",
    "    answer = [ans['text'][0] for ans in dataset['answers'][:size]]\n",
    "\n",
    "    return {'context': context, 'question': question, 'answer': answer}\n",
    "\n",
    "def run_inference():\n",
    "    size = 300\n",
    "    squad_like_data = squad_question(size)\n",
    "    accuracy = 0\n",
    "\n",
    "    for i in range(size):\n",
    "        context_sentences = nltk.sent_tokenize(squad_like_data['context'][i])\n",
    "        question = squad_like_data['question'][i]\n",
    "        answer = squad_like_data['answer'][i]\n",
    "        bert_answer = transformer_local_search(question, context_sentences)\n",
    "\n",
    "        if answer in bert_answer:\n",
    "            accuracy += 1\n",
    "\n",
    "    return accuracy / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a40ec29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.64\n"
     ]
    }
   ],
   "source": [
    "acc = run_inference()\n",
    "print(f\"Accuracy : {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a40aefc",
   "metadata": {},
   "source": [
    "The information about the data used to train the model is usually mentionned on the description of the dataset or in the logs. In case, the list of the datas on what the model is trained is not available publicly.\n",
    "\n",
    "We can use the above method. First, we send a sample of questions to our pre-trained model. Then we calculate the accuracy of the predicted response and the ground truth. However, it's difficult to know whether the dataset is or isn't used to train the model.\n",
    "\n",
    "Over our sample, the accuracy is equal to 0.64, this is quite low. We can suppose that the model is not trained an SQuAD dataset.\n",
    "\n",
    "Finally, without the official information of the data used to train it is difficult to know if our pre-trained model is trained on SQuAD dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892754d3",
   "metadata": {},
   "source": [
    "#### Fine Tuned BERT model\n",
    "##### Preprocessing\n",
    "\n",
    "To helps us with the preporcessing we used an exemple on the Huggign face website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d89b2137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering\n",
    "\n",
    "# DistillBert\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "dataset = load_dataset('squad')\n",
    "\n",
    "def process_data(batch):\n",
    "    answer_starts = []\n",
    "    answer_ends = []\n",
    "    batch_size = len(batch['question'])\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        answer_start = batch['answers'][i]['answer_start'][0]\n",
    "        answer_text = batch['answers'][i]['text'][0]\n",
    "        answer_end = answer_start + len(answer_text)\n",
    "        answer_starts.append(answer_start)\n",
    "        answer_ends.append(answer_end)\n",
    "\n",
    "    # input_ids\n",
    "    tokenized_inputs = tokenizer(batch['question'], batch['context'], truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "    tokenized_inputs.update({'start_positions': answer_starts, 'end_positions': answer_ends})\n",
    "\n",
    "    attention_masks = tokenized_inputs['attention_mask']\n",
    "    batch['attention_masks'] = torch.tensor(attention_masks)\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0815e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset['train'].map(process_data, batched=True)\n",
    "val_dataset = dataset['validation'].map(process_data, batched=True)\n",
    "train_dataset.set_format(type='torch', columns=['id', 'input_ids', 'attention_mask', 'start_positions', 'end_positions'])\n",
    "val_dataset.set_format(type='torch', columns=['id', 'input_ids', 'attention_mask', 'start_positions', 'end_positions'])\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efab584c",
   "metadata": {},
   "source": [
    "We only run the fine tune on 1 epoch due to the long training time (1h30/2h)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a692e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'optimizer = AdamW(model.parameters(), lr=5e-5)\\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\\n\\nfor epoch in range(1):\\n    for batch in train_loader:\\n        input_ids = batch[\\'input_ids\\'].to(device)\\n        start_positions = batch[\\'start_positions\\'].to(device)\\n        attention_mask = batch[\\'attention_mask\\'].to(device)\\n        start_positions = batch[\\'start_positions\\'].to(device)\\n        end_positions = batch[\\'end_positions\\'].to(device)\\n\\n        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\\n        loss = outputs[0]\\n\\n        if torch.isnan(loss):\\n            continue\\n\\n        # Backward\\n        optimizer.zero_grad()\\n        loss.backward()\\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\\n        optimizer.step()\\n\\n        # Display training information\\n        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\\n\\nmodel.eval()'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Display training information\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcd382c",
   "metadata": {},
   "source": [
    "##### Save the fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "922972c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'fine_tuned_bert_model_full.pth')\n",
    "torch.save(model.state_dict(), 'fine_tuned_bert_model_state_dict.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d7855",
   "metadata": {},
   "source": [
    "##### Use fine tuned model\n",
    "\n",
    "We saved our fine tuned model : [Download Link](https://drive.google.com/file/d/1-bBx7A_xkj9RgWen_3KF_Nrkb27dwUv4/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "599047d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
    "fine_tuned_model.load_state_dict(torch.load('fine_tuned_bert_model_state_dict.pth'))\n",
    "fine_tuned_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2295fe",
   "metadata": {},
   "source": [
    "##### Evaluate fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e17f32fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output answer matches expected answer:  False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.0001058857305906713,\n",
       " 'start': 116,\n",
       " 'end': 163,\n",
       " 'answer': '2015 season. The American Football Conference ('}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "example = dataset[\"validation\"][0]\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=fine_tuned_model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "output = qa_pipeline({\n",
    "    \"question\": example[\"question\"],\n",
    "    \"context\": example[\"context\"]\n",
    "})\n",
    "answer_text = example[\"answers\"][\"text\"][0]\n",
    "print(\"output answer matches expected answer: \", output[\"answer\"] == answer_text)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c2cea30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuned accuracy : 0.006\n"
     ]
    }
   ],
   "source": [
    "def fine_tuned_eval(dataset):\n",
    "    correct = 0\n",
    "    size = 500\n",
    "    for i in range(size):\n",
    "        sample = dataset[i]\n",
    "        qa_pipeline = pipeline(\n",
    "            \"question-answering\",\n",
    "            model=fine_tuned_model,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "        output = qa_pipeline({\n",
    "            \"question\": sample[\"question\"],\n",
    "            \"context\": sample[\"context\"]\n",
    "        })\n",
    "        answer_text = sample[\"answers\"][\"text\"][0]\n",
    "\n",
    "        if output[\"answer\"] in answer_text:\n",
    "            correct += 1\n",
    "\n",
    "    return correct/size\n",
    "\n",
    "print('Fine tuned accuracy :', fine_tuned_eval(dataset['validation']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126e965a",
   "metadata": {},
   "source": [
    "When we run the fine tuned model, the loss after one epoch is 5. That may explain why, the accuracy is extremely low. We should run the fine tunning over more epoch, the second epoch return a loss of 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8d3698",
   "metadata": {},
   "source": [
    "## 3 - Local search on another dataset: does it work ? \n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "            \n",
    "Let's implement our local semantic search on another dataset, to check if performance follows the same trend. You can use the [```commonsense_qa``` dataset](https://huggingface.co/datasets/commonsense_qa). Do the same exploration and explanation you did for the SQuAD task. How is this dataset different ? \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>\n",
    "            \n",
    "Look at the data and apply the same two approaches you did before. What do you observe ? Propose an explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02e30e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05fb9ccba24a4bd6b25ab7d75e2f3503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.64k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23244a3d81134b12b11b0539ac0d1b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/3.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5706f181dd5a4a81854be247f51d9905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466cddf6a5d34f04869ede1ca845f63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c23e38e89e4fa4a7fd670cdab096c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.79M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2021af04eb764b3ea8d0d90ac2bfe753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/472k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942118d815d24b80b3b9212881df4cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/423k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be56cba3c904fc49a7fef54f71500be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad055bae8f284adda653b9be0de8c44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9741 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276f027d0ee8471d8d46a8a51ccb13d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25da951fc774d24875f205602b8ec1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 9741\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 1221\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
      "        num_rows: 1140\n",
      "    })\n",
      "})\n",
      "{'id': '4c1cb0e95b99f72d55c068ba0255c54d', 'question': 'To locate a choker not located in a jewelry box or boutique where would you go?', 'question_concept': 'choker', 'choices': {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['jewelry store', 'neck', 'jewlery box', 'jewelry box', 'boutique']}, 'answerKey': 'A'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Load dataset\n",
    "dataset_commonsense = load_dataset(\"commonsense_qa\")\n",
    "print(dataset_commonsense)\n",
    "\n",
    "print(dataset_commonsense['train'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e056224",
   "metadata": {},
   "source": [
    "Looking at the dataset, we observe that it has a structure similar to the SQuAD dataset, with a question and possible answers. The answers are already separated. \n",
    "\n",
    "Without applying the same approaches as before, by examining an example from the dataset, we can conclude that our local search will not work because the answers are not contained within the question.\n",
    "\n",
    "\n",
    "#### Local search : Tf-IDF representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0aa68f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Valen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Model Accuracy on Commonsense Dataset: 0.17526617526617527\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "\n",
    "# Download the punkt tokenizer for sentence tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tfidf_local_search(example):\n",
    "  \n",
    "    choices = example['choices']['text']\n",
    "    question = example['question']\n",
    "\n",
    "    # Combine the question and choices for vectorization\n",
    "    texts = [question] + choices\n",
    "\n",
    "    # Create a tf-idf vectorizer and fit it on the texts\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "    # tf-idf representation for the question and choices\n",
    "    question_vec = tfidf_matrix[0]\n",
    "    choices_vecs = tfidf_matrix[1:]\n",
    "\n",
    "    # Compute cosine similarities between question and each choice\n",
    "    similarities = cosine_similarity(question_vec, choices_vecs)\n",
    "    most_similar_idx = similarities.argsort()[0][-1]\n",
    "\n",
    "    return example['choices']['label'][most_similar_idx]\n",
    "\n",
    "def evaluate_tfidf_accuracy(dataset):\n",
    "    correct = 0\n",
    "\n",
    "    num_samples = len(dataset['validation'])\n",
    "    for i in range(num_samples):\n",
    "        example = dataset['validation'][i]\n",
    "        answer = example['answerKey']\n",
    "        predicted_choice_label = tfidf_local_search(example)\n",
    "        if answer == predicted_choice_label:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / num_samples\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "dataset_commonsense = load_dataset(\"commonsense_qa\")\n",
    "accuracy = evaluate_tfidf_accuracy(dataset_commonsense)\n",
    "print(\"TF-IDF Model Accuracy on Commonsense Dataset:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b81429",
   "metadata": {},
   "source": [
    "The accuracy is at 0.1, which is quite low. We obtain these results because in our dataset, the correct answer is not contained within the question, or a false answer is present in the question.\n",
    "\n",
    "For instance, consider a question like 'To locate a choker not located in a jewelry box or boutique, where would you go?' with possible answers ['jewelry store', 'neck', 'jewelry box', 'jewelry box', 'boutique'], and the correct answer is 'jewelry store'. Our model selects the answer with the highest similarity to the question, so in this case, it would be misled and choose 'jewelry box'.\n",
    "\n",
    "A model designed to predict answers should be more accurate, consider negations, and grasp the true meaning of the question to respond appropriately.\n",
    "\n",
    "\n",
    "\n",
    "#### Local search with a pre trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a79ce501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2858312858312858\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Load the commonsense_qa dataset\n",
    "dataset_commonsense = load_dataset(\"commonsense_qa\")\n",
    "\n",
    "def vectorize(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    return torch.mean(outputs.last_hidden_state, dim=1)[0]\n",
    "\n",
    "def transformer_local_search(question, choices):\n",
    "    # Vectorize the question\n",
    "    question_vec = vectorize(question, tokenizer, model).detach().numpy()\n",
    "\n",
    "    # Vectorize choices and compute cosine similarities\n",
    "    sims = []\n",
    "    for choice in choices['text']:\n",
    "        choice_vec = vectorize(choice, tokenizer, model).detach().numpy()\n",
    "        sim = 1 - cosine(choice_vec, question_vec)\n",
    "        sims.append(sim)\n",
    "\n",
    "    # Find the most similar choice\n",
    "    return choices['label'][np.argmax(sims)]\n",
    "\n",
    "\n",
    "def evaluate_transformer_local_search_accuracy(dataset):\n",
    "    correct = 0\n",
    "\n",
    "    num_samples = len(dataset['validation'])\n",
    "    for i in range(num_samples):\n",
    "        example = dataset['validation'][i]\n",
    "        answer = example['answerKey']\n",
    "        predicted_choice_label = transformer_local_search(example['question'], example['choices'])\n",
    "        if answer == predicted_choice_label:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / num_samples\n",
    "    return accuracy\n",
    "\n",
    "# Example usage\n",
    "accuracy = evaluate_transformer_local_search_accuracy(dataset_commonsense)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56a46ac",
   "metadata": {},
   "source": [
    "As we guessed, even with a pre-trained model, accuracy is very low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569ed206",
   "metadata": {},
   "source": [
    "## 4 - Global search on Wikipedia data\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "            \n",
    "Again, look at the data of the [```wiki_qa``` dataset](https://huggingface.co/datasets/wiki_qa), understand the task. We are now going to perform a **global** search, as the dataset is open domain: when trying to answer for a question, we will search among all vectors, rather than only the ones representing the context the answer is found in. How would you verify that the model managed to find the right answer ? Let's try to use to very different ways to evaluate how well the approaches work:\n",
    "- Looking if the right result is in the top-$k$ predictions returned by the model.\n",
    "- Using the [ROUGE](https://aclanthology.org/W04-1013/) score. \n",
    "Explain how you understand these metrics and how they could be useful here.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>\n",
    "            \n",
    "We will use the same embeddings as before, but we will use a tool called ```faiss``` for indexing all of them and facilitate the search ! Look at the [documentation](https://huggingface.co/docs/datasets/faiss_es). Then, implement or use tools implementing the two metrics, and evaluate both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f261e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question_id', 'question', 'document_title', 'answer', 'label'],\n",
      "    num_rows: 2733\n",
      "})\n",
      "{'question_id': 'Q8', 'question': 'How are epithelial tissues joined together?', 'document_title': 'Tissue (biology)', 'answer': 'Cross section of sclerenchyma fibers in plant ground tissue', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "wiki_dataset = load_dataset(\"wiki_qa\", split='validation')\n",
    "print(wiki_dataset)\n",
    "print(wiki_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66455836",
   "metadata": {},
   "source": [
    "A faiss index is used for indexing and searching a set of vectors.  FAISS is a library for dense retrieval. It retrieves documents based on their vector representations, by doing a nearest neighbors search. It will allow us to search through all the answers to find the one closest to the question. We need to first give a vector representation of our elements.\n",
    "\n",
    "Then, we send our query and the number of results we want to the .search() method. It will return the k vectors closest to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f87588d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04525ea",
   "metadata": {},
   "source": [
    "Create the vectorize method to embed the elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d38d3f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    return torch.mean(outputs.last_hidden_state, dim=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97578228",
   "metadata": {},
   "source": [
    "Now we build our faiss embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6daf427",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_faiss_index(dataset, model, tokenizer):\n",
    "    \n",
    "    # Generate embeddings for all answers\n",
    "    answer_embeddings = []\n",
    "\n",
    "    for answer in dataset['answer']:\n",
    "        inputs = tokenizer(answer, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        outputs = model(**inputs)\n",
    "        answer_embedding = torch.mean(outputs.last_hidden_state, dim=1)[0].detach().numpy()\n",
    "        answer_embeddings.append(answer_embedding)\n",
    "\n",
    "    # Convert embeddings to NumPy array for Faiss\n",
    "    answer_embeddings = np.array(answer_embeddings)\n",
    "\n",
    "    # Build Faiss index\n",
    "    index = faiss.IndexFlatL2(answer_embeddings.shape[1])\n",
    "    index.add(answer_embeddings)\n",
    "    return index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7106e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_search_top_k(k, dataset, model, tokenizer, faiss_index, question):\n",
    "    # Embed the question\n",
    "    new_question_embedding = vectorize(question, tokenizer, model).detach().numpy()\n",
    "    # Search for the k closest vectors\n",
    "    distances, indices = faiss_index.search(new_question_embedding.reshape(1, -1), k)\n",
    "    # Retrieve the top-K answers using indices\n",
    "    top_k_answers = [dataset['answer'][i] for i in indices[0]]\n",
    "\n",
    "    return top_k_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1d4690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_correct_answer(question, dataset):\n",
    "    question_indexes = []\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset[i]['question'] == question:\n",
    "            question_indexes.append(i)\n",
    "            \n",
    "    labels = []\n",
    "    for i in question_indexes:\n",
    "        labels.append(dataset[i]['label'])\n",
    "\n",
    "    # Check if labels if not empty\n",
    "    if labels:\n",
    "        correct_answer_index = np.argmax(labels)\n",
    "        if correct_answer_index == 1:\n",
    "            return dataset['label'][correct_answer_index]\n",
    "    return False\n",
    "\n",
    "def check_answer(top_k_answers, question):\n",
    "    correct_answer = find_correct_answer(question, dataset['validation'])\n",
    "    if correct_answer is not False:\n",
    "        for answer in top_k_answers:\n",
    "            if correct_answer in answer:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2bd619",
   "metadata": {},
   "source": [
    "Evaluation of the model using the k-top predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50ba94c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Organs are then formed by the functional grouping together of multiple tissues.', 'Changes to the oxygen-absorbing tissues', 'Cross section of sclerenchyma fibers in plant ground tissue', 'Alveoli are particular to mammalian lungs.', 'The alveolar membrane is the gas-exchange surface.']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "wiki_dataset = load_dataset(\"wiki_qa\", split='validation')\n",
    "\n",
    "# Build Faiss index\n",
    "faiss_index = build_faiss_index(wiki_dataset, model, tokenizer)\n",
    "\n",
    "# Perform global search\n",
    "question = wiki_dataset[0]['question']\n",
    "top_k_answers = global_search_top_k(5, wiki_dataset, model, tokenizer, faiss_index, question)\n",
    "print(top_k_answers)\n",
    "\n",
    "res = check_answer(top_k_answers, question)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc652f0",
   "metadata": {},
   "source": [
    "### Using the rouge score\n",
    "\n",
    "The rouge score ROUGE return metrics range between 0 and 1, with higher scores indicating higher similarity between the automatically produced summary and the reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "35552687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference Answer: Not found\n",
      "Retrieved Answers: ['Organs are then formed by the functional grouping together of multiple tissues.', 'Changes to the oxygen-absorbing tissues', 'Cross section of sclerenchyma fibers in plant ground tissue', 'Alveoli are particular to mammalian lungs.', 'The alveolar membrane is the gas-exchange surface.']\n",
      "ROUGE Scores: {'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "def find_answer_text(wiki_dataset, question):\n",
    "    for i in range(len(wiki_dataset)):\n",
    "        if wiki_dataset[i]['question'] == question and wiki_dataset[i]['label'] == 1:\n",
    "            return wiki_dataset[i]['answer']\n",
    "        else:\n",
    "            return 'Not found'\n",
    "\n",
    "def compute_rouge_scores(retrieved_answers, reference_answers):\n",
    "    # Initialize the Rouge scoring object\n",
    "    rouge = Rouge()\n",
    "\n",
    "    if isinstance(retrieved_answers, str):\n",
    "        retrieved_answers = [retrieved_answers]\n",
    "    if isinstance(reference_answers, str):\n",
    "        reference_answers = [reference_answers]\n",
    "\n",
    "    if not retrieved_answers or not reference_answers:\n",
    "        print(\"One or both of the answer sets are empty.\")\n",
    "        return None\n",
    "\n",
    "    # Prepare the answers for Rouge evaluation\n",
    "    retrieved_concatenated = ' '.join(retrieved_answers)\n",
    "    reference_concatenated = ' '.join(reference_answers)\n",
    "\n",
    "    try:\n",
    "        # Compute ROUGE scores\n",
    "        scores = rouge.get_scores(retrieved_concatenated, reference_concatenated, avg=True)\n",
    "        return scores\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating ROUGE scores: {e}\")\n",
    "        return None\n",
    "\n",
    "reference_answers = find_answer_text(wiki_dataset, question)\n",
    "\n",
    "print(\"Reference Answer:\", reference_answers)\n",
    "print(\"Retrieved Answers:\", top_k_answers)\n",
    "\n",
    "# Compute ROUGE scores\n",
    "rouge_scores = compute_rouge_scores(top_k_answers, reference_answers)\n",
    "print(\"ROUGE Scores:\", rouge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdc4241",
   "metadata": {},
   "source": [
    "## 5 - BONUS: Run a model on the original Squad task\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>\n",
    "\n",
    "Of course, we need to know that you understood the code: simplify it to the maximum (only what's necessary to obtain predictions) and comment abundantly ! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d354e86",
   "metadata": {},
   "source": [
    "There's a lot of models that we could use for this bonus question. After few searches online, we did find that BERT if the most accurate model for Squad task. But choosing a model often depends on the balance that we want to have between accuracy and computational ressources. Since, they are a lot of versions of BERT depending on this topic we would like to test the difference between 2 of those models.\n",
    "\n",
    "From what we saw, **RoBERTa** is SOTA for this task. As we saw in class, it is a BERT model that was trained for longer and with more data. Thus, it is a more accurate model but more complex which include more computational ressources.\n",
    "\n",
    "We will then try to see the difference between this model and **DistilBERT** which is a distilled version of BERT that retains most of BERT’s performance but with fewer parameters and faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5b0c70c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c49ac",
   "metadata": {},
   "source": [
    "#### Generic functions to load the model, tokenizer and get the predictions for Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "deed886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a model and its tokenizer\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "# Function to get predictions from the model\n",
    "def get_prediction(model, tokenizer, context, question):\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        answer_start_scores = outputs.start_logits\n",
    "        answer_end_scores = outputs.end_logits\n",
    "\n",
    "        # Finds the tokens with the highest 'start' and 'end' scores\n",
    "        answer_start = torch.argmax(answer_start_scores)\n",
    "        answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "        # Convert tokens to the answer string\n",
    "        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1efa8c2",
   "metadata": {},
   "source": [
    "#### Function to compute the accuracy of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0cc63dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(dataset, model, tokenizer, num_samples=100):\n",
    "    correct = 0\n",
    "\n",
    "    for item in dataset.select(range(num_samples)):\n",
    "        context = item['context']\n",
    "        question = item['question']\n",
    "        # SQuAD has multiple answers, here we use the first answer as an example\n",
    "        actual_answer = item['answers']['text'][0] if item['answers']['text'] else ''\n",
    "\n",
    "        predicted_answer = get_prediction(model, tokenizer, context, question)\n",
    "\n",
    "        # Increments correct count if the prediction matches the actual answer\n",
    "        if predicted_answer.lower().strip() == actual_answer.lower().strip():\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / num_samples\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d00e3381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the dataset for safety measures\n",
    "squad_dataset = load_dataset(\"squad\")\n",
    "\n",
    "# Choosing the question we want to answer with it's associated context\n",
    "question_to_answer = 0\n",
    "context = squad_dataset['validation'][question_to_answer]['context']\n",
    "question = squad_dataset['validation'][question_to_answer]['question']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42e80c5",
   "metadata": {},
   "source": [
    "**Remark :** We we'll use only a sample of the dataset to compute the accuracies for computational gains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5380d4fa",
   "metadata": {},
   "source": [
    "### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b14fd089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab6cf45cee44c4985272a120abd75f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Valen\\anaconda3\\envs\\pml\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Valen\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8486d8eb1abd40a9988e0c5b534417a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/451 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1686d452432d44059fb066486089d7fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a4e4e2012a46629cb43f9273612977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db45a42bad4d4ffab07ca8e81bead42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading the model and tokenizer for DistilBERT\n",
    "distilbert_tokenizer, distilbert_model = load_model_and_tokenizer(\"distilbert-base-uncased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "90f1b83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question :  Which NFL team represented the AFC at Super Bowl 50?\n",
      "\n",
      "DistilBERT anwser : denver broncos\n"
     ]
    }
   ],
   "source": [
    "# Testing DistilBERT on SQuAD\n",
    "answer = get_prediction(distilbert_model, distilbert_tokenizer, context, question)\n",
    "print(\"Question : \", question)\n",
    "print(\"\\nDistilBERT anwser :\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6f57ffd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT Accuracy: 64.20%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of DistilBERT\n",
    "distilbert_accuracy = compute_accuracy(squad_dataset['validation'], distilbert_model, distilbert_tokenizer, 500)\n",
    "print(f\"DistilBERT Accuracy: {distilbert_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89df1e",
   "metadata": {},
   "source": [
    "#### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "df443cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993795c3a23f4b3a88d175233f3a156c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c42e15fa0a4ede8576e93f567f40bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee2f781f1cd4478951951d1ea23ee74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc0b50bb3ef4f0f89bd2c923fdb93e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ffcb1158d7415d8486190a9a431502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0251c284d82943258073a04ba76c1342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading the model and tokenizer for RoBERTa\n",
    "roberta_tokenizer, roberta_model = load_model_and_tokenizer(\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b6eb2382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question :  Which NFL team represented the AFC at Super Bowl 50?\n",
      "\n",
      "RoBERTa answer :  Denver Broncos\n"
     ]
    }
   ],
   "source": [
    "# Testing RoBERTa on SQuAD\n",
    "answer = get_prediction(roberta_model, roberta_tokenizer, context, question)\n",
    "print(\"Question : \", question)\n",
    "print(\"\\nRoBERTa answer :\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "963fc7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa Accuracy: 75.40%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of RoBERTa\n",
    "distilbert_accuracy = compute_accuracy(squad_dataset['validation'], roberta_model, roberta_tokenizer, 500)\n",
    "print(f\"RoBERTa Accuracy: {distilbert_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f4aad0",
   "metadata": {},
   "source": [
    "**Conclusion :** As planned, RoBERTa is more accurate than DistilBERT for the 500 samples we took. However, it is also much longer to process the samples with almost twice more time to process. The balance between accuracy and computational ressources is thus a very important parameter to take into account in the NLP field. SOTA models are not always suited of every task as they are often more complex and longer to run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
